{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db4df52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2326, -0.2683, -0.3636,  0.0406,  0.1745],\n",
       "        [-0.3909,  0.0048,  0.4311,  0.1305,  0.1588],\n",
       "        [-0.2086,  0.0812, -0.3699, -0.2499, -0.3439],\n",
       "        [-0.3744,  0.1706,  0.2203,  0.1285, -0.4332],\n",
       "        [-0.2873,  0.3833,  0.0866,  0.2926,  0.1639]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nn.Linear(5, 5)\n",
    "x.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd2bed7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0886,  0.0299,  1.3401, -1.6356, -1.5045],\n",
       "        [-1.0365, -0.6577, -0.8844, -0.3822,  0.2974],\n",
       "        [ 0.3497,  0.2246, -0.2943, -0.7901,  0.3882],\n",
       "        [ 0.4695, -0.0967,  0.1978,  1.2718,  1.4428],\n",
       "        [ 0.3492,  1.1956,  2.3448, -0.5843, -0.9778]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(x.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f5c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b964b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43f52f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset online_news_popularity_data (/home/leepark/.cache/huggingface/datasets/online_news_popularity_data/online_news_popularity_data/1.0.0/f3e03630a13ebe013884d6a83c7ec52cb4eec2c0f6012f710c9dba58aa719fcd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d354072fad4186a825f9aff342e96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "articles = load_dataset('online_news_popularity_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d0a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d175cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    # full_text_encoded = tokenizer([title + ' \\n' + content for title, content in zip(batch['title'],batch['content'])],\n",
    "    #                              return_tensors = 'pt', padding = 'max_length', max_length = 512, truncation = True)\n",
    "    title_encoded = tokenizer(batch['title'], padding = True,  truncation = True)\n",
    "    title_encoded_renamed = {f\"{k}_title\":v for k,v in title_encoded.items()}\n",
    "    content_encoded = tokenizer(batch['content'], padding = 'max_length', max_length = 512, truncation = True)\n",
    "    content_encoded_renamed = {f\"{k}_content\":v for k,v in content_encoded.items()}\n",
    "#     return {k:torch.Tensor(v) for k,v in full_text_encoded.items()}\n",
    "    title_encoded_renamed.update(content_encoded_renamed)\n",
    "    return title_encoded_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0691204e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles_encoded = articles.map(tokenize, remove_columns = ['title','content','shares'],\n",
    "            batched = True, batch_size = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4053585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from Transformer_Models import ContextDistilBert, ContextDistilBertwithData\n",
    "import torch\n",
    "\n",
    "\n",
    "model_ckpt = 'domain_adaptation_final_body'\n",
    "\n",
    "encoder = ContextDistilBertwithData(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79c26d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_original = articles.remove_columns(['title','content','shares'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "0162eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = articles_original['train'].with_format('pt')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "17573acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.cat([v.reshape(-1, 1) for k,v in data_dict.items() if 'shares' not in k], dim = -1)\n",
    "input_tensor = torch.vstack([torch.flatten(torch.kron(v, v)) for v in input_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "203c08c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2809])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "e20e9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class NN_Classifier_Output:\n",
    "    loss: torch.FloatTensor = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    preds: np.array = None\n",
    "    labels: torch.FloatTensor = None\n",
    "\n",
    "class simple_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(2809, 2809*2)\n",
    "        self.dropout = nn.Dropout(.15)\n",
    "        self.relu   = nn.GELU()\n",
    "        self.linear_2 = nn.Linear(2809*2, 2809)\n",
    "        self.linear_3 = nn.Linear(2809, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        input_tensor = torch.cat([v.reshape(-1, 1) for k,v in kwargs.items() if 'shares' not in k], dim = -1)\n",
    "        input_tensor = torch.vstack([torch.flatten(torch.kron(v, v)) for v in input_tensor])\n",
    "        labels = kwargs['shares_class'].long()\n",
    "        output = self.linear_1(F.normalize(input_tensor))\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear_2(output)\n",
    "        logits = self.linear_3(self.dropout(self.relu(output)))\n",
    "        \n",
    "        \n",
    "        softmax = F.softmax(logits, dim = -1).detach().numpy()\n",
    "        \n",
    "#         calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction = 'sum', label_smoothing = .02)\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        result = NN_Classifier_Output(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            preds = softmax,\n",
    "            labels = labels\n",
    "        )\n",
    "        \n",
    "        # return model output object\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "0f99635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_model = simple_NN()\n",
    "optimizer = torch.optim.AdamW(nn_model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "3c645ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_loop(h_training_dataset, model, optimizer, batch_size, verbose = False):\n",
    "    h_training_dataset = h_training_dataset.with_format('pt').shuffle()\n",
    "    total_rows = h_training_dataset.num_rows\n",
    "    steps = total_rows // batch_size\n",
    "    if verbose:\n",
    "        pbar = tqdm(range(steps +1))\n",
    "        for batch_idx in pbar:\n",
    "            start = batch_idx*batch_size; end = batch_idx*batch_size + batch_size\n",
    "            if end > total_rows:\n",
    "                result = model(**{k:v for k,v in h_training_dataset[start:].items()})\n",
    "            else:\n",
    "                result = model(**{k:v for k,v in h_training_dataset[start:end].items()})\n",
    "            optimizer.zero_grad()\n",
    "            result.loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                loss, current = result.loss.item(), (batch_idx + 1) * steps\n",
    "                pbar.set_postfix({\"mean(loss)\":\"\"f\"{loss:>7f}  [{current:>5d}/{batch_size:>5d}]\"})\n",
    "    else:\n",
    "        for batch_idx in range(steps +1):\n",
    "            start = batch_idx*batch_size; end = batch_idx*batch_size + batch_size\n",
    "            if end > total_rows:\n",
    "                result = model(**{k:v for k,v in h_training_dataset[start:].items()})\n",
    "            else:\n",
    "                result = model(**{k:v for k,v in h_training_dataset[start:end].items()})\n",
    "            optimizer.zero_grad()\n",
    "            result.loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 25 == 0:\n",
    "                loss, current = result.loss.item(), (batch_idx + 1) * batch_size\n",
    "                print({\"mean(loss)\":\"\"f\"{loss:>7f}  [{current:>5d}/{total_rows:>5d}]\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "6486df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def compute_metrics(labels, preds):\n",
    "    f1 = f1_score(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall':recall, 'f1':f1, 'auc':auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "7ff87d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(h_test_dataset, model, batch_size, verbose = False):\n",
    "    h_test_dataset = h_test_dataset.with_format('pt').shuffle()\n",
    "    total_rows = h_test_dataset.num_rows\n",
    "    steps = total_rows // batch_size\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    preds = []\n",
    "    labels = h_test_dataset.with_format('np')['shares_class']\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        if verbose:\n",
    "            pbar = tqdm(range(steps +1))\n",
    "            for batch_idx in pbar:\n",
    "                start = batch_idx*batch_size; end = batch_idx*batch_size + batch_size\n",
    "                if end > total_rows:\n",
    "                    result = model(**{k:v for k,v in h_test_dataset[start:].items()})\n",
    "                else:\n",
    "                    result = model(**{k:v for k,v in h_test_dataset[start:end].items()})\n",
    "                loss += result.loss.item()\n",
    "                preds.append(np.argmax(result.preds, -1))\n",
    "            preds = np.concatenate(preds)\n",
    "            accuracy = accuracy_score(preds.reshape(-1,1), labels.reshape(-1,1))\n",
    "            loss /= total_rows\n",
    "            print(f\"accuracy : {accuracy}\")\n",
    "            print(f\"loss : {loss}\")\n",
    "            return compute_metrics(labels, preds)\n",
    "        else:\n",
    "            for batch_idx in range(steps +1):\n",
    "                start = batch_idx*batch_size; end = batch_idx*batch_size + batch_size\n",
    "                if end > total_rows:\n",
    "                    result = model(**{k:v for k,v in h_test_dataset[start:].items()})\n",
    "                else:\n",
    "                    result = model(**{k:v for k,v in h_test_dataset[start:end].items()})\n",
    "                loss += result.loss.item()\n",
    "                preds.append(np.argmax(result.preds, -1))\n",
    "            preds = np.concatenate(preds)\n",
    "            accuracy = accuracy_score(preds.reshape(-1,1), labels.reshape(-1,1))\n",
    "            loss /= total_rows\n",
    "            print(f\"evaluation result : accuracy : {accuracy}, loss : {loss}\")\n",
    "            return compute_metrics(labels, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "7fa43345",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean(loss)': '66.784706  [  100/31686]'}\n",
      "{'mean(loss)': '69.858818  [ 2600/31686]'}\n",
      "{'mean(loss)': '67.101128  [ 5100/31686]'}\n",
      "{'mean(loss)': '66.627663  [ 7600/31686]'}\n",
      "{'mean(loss)': '67.919441  [10100/31686]'}\n",
      "{'mean(loss)': '65.001122  [12600/31686]'}\n",
      "{'mean(loss)': '68.053848  [15100/31686]'}\n",
      "{'mean(loss)': '65.664352  [17600/31686]'}\n",
      "{'mean(loss)': '69.843575  [20100/31686]'}\n",
      "{'mean(loss)': '70.365585  [22600/31686]'}\n",
      "{'mean(loss)': '68.642189  [25100/31686]'}\n",
      "{'mean(loss)': '68.711418  [27600/31686]'}\n",
      "{'mean(loss)': '67.622452  [30100/31686]'}\n",
      "evaluation result : accuracy : 0.5684170663973744, loss : 0.6787905020113095\n",
      "{'mean(loss)': '68.190559  [  100/31686]'}\n",
      "{'mean(loss)': '67.504478  [ 2600/31686]'}\n",
      "{'mean(loss)': '69.684685  [ 5100/31686]'}\n",
      "{'mean(loss)': '66.440605  [ 7600/31686]'}\n",
      "{'mean(loss)': '64.930801  [10100/31686]'}\n",
      "{'mean(loss)': '67.543205  [12600/31686]'}\n",
      "{'mean(loss)': '66.385651  [15100/31686]'}\n",
      "{'mean(loss)': '68.732430  [17600/31686]'}\n",
      "{'mean(loss)': '67.060730  [20100/31686]'}\n",
      "{'mean(loss)': '68.357872  [22600/31686]'}\n",
      "{'mean(loss)': '65.999321  [25100/31686]'}\n",
      "{'mean(loss)': '70.803070  [27600/31686]'}\n",
      "{'mean(loss)': '67.568703  [30100/31686]'}\n",
      "evaluation result : accuracy : 0.5737187578894218, loss : 0.6786162105060473\n",
      "{'mean(loss)': '69.233665  [  100/31686]'}\n",
      "{'mean(loss)': '67.705978  [ 2600/31686]'}\n",
      "{'mean(loss)': '68.367676  [ 5100/31686]'}\n",
      "{'mean(loss)': '69.739990  [ 7600/31686]'}\n",
      "{'mean(loss)': '68.263725  [10100/31686]'}\n",
      "{'mean(loss)': '68.385323  [12600/31686]'}\n",
      "{'mean(loss)': '69.936958  [15100/31686]'}\n",
      "{'mean(loss)': '68.883629  [17600/31686]'}\n",
      "{'mean(loss)': '67.694542  [20100/31686]'}\n",
      "{'mean(loss)': '69.443802  [22600/31686]'}\n",
      "{'mean(loss)': '70.682373  [25100/31686]'}\n",
      "{'mean(loss)': '66.370621  [27600/31686]'}\n",
      "{'mean(loss)': '66.441971  [30100/31686]'}\n",
      "evaluation result : accuracy : 0.5716990658924515, loss : 0.6781426102065944\n",
      "{'mean(loss)': '67.599442  [  100/31686]'}\n",
      "{'mean(loss)': '64.337769  [ 2600/31686]'}\n",
      "{'mean(loss)': '69.573402  [ 5100/31686]'}\n",
      "{'mean(loss)': '67.627403  [ 7600/31686]'}\n",
      "{'mean(loss)': '67.222633  [10100/31686]'}\n",
      "{'mean(loss)': '65.918739  [12600/31686]'}\n",
      "{'mean(loss)': '66.349464  [15100/31686]'}\n",
      "{'mean(loss)': '69.964302  [17600/31686]'}\n",
      "{'mean(loss)': '67.643326  [20100/31686]'}\n",
      "{'mean(loss)': '70.346397  [22600/31686]'}\n",
      "{'mean(loss)': '69.494217  [25100/31686]'}\n",
      "{'mean(loss)': '66.717628  [27600/31686]'}\n",
      "{'mean(loss)': '68.861198  [30100/31686]'}\n",
      "evaluation result : accuracy : 0.5724564503913153, loss : 0.6783092780957105\n",
      "{'mean(loss)': '66.952431  [  100/31686]'}\n",
      "{'mean(loss)': '71.955025  [ 2600/31686]'}\n",
      "{'mean(loss)': '67.184937  [ 5100/31686]'}\n",
      "{'mean(loss)': '68.653931  [ 7600/31686]'}\n",
      "{'mean(loss)': '69.701454  [10100/31686]'}\n",
      "{'mean(loss)': '66.986618  [12600/31686]'}\n",
      "{'mean(loss)': '68.651550  [15100/31686]'}\n",
      "{'mean(loss)': '66.459923  [17600/31686]'}\n",
      "{'mean(loss)': '66.447388  [20100/31686]'}\n",
      "{'mean(loss)': '66.313972  [22600/31686]'}\n",
      "{'mean(loss)': '66.285477  [25100/31686]'}\n",
      "{'mean(loss)': '70.479836  [27600/31686]'}\n",
      "{'mean(loss)': '70.436348  [30100/31686]'}\n",
      "evaluation result : accuracy : 0.5727089118909366, loss : 0.6790848645078086\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import \n",
    "\n",
    "optimizer = torch.optim.AdamW(nn_model.parameters(), lr = 5e-1)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 1, 1e-5, total_iters = 7)\n",
    "\n",
    "for i in range(5):\n",
    "    train_loop(articles_original['train'], nn_model, optimizer, 100)\n",
    "    test_loop(articles_original['validation'], nn_model, 500)\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "364b529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation result : accuracy : 0.5748548346377177, loss : 0.6786144076498792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5748548346377177,\n",
       " 'precision': 0.5687228052213975,\n",
       " 'recall': 0.5690140845070423,\n",
       " 'f1': 0.568868407578085,\n",
       " 'auc': 0.574773410189792}"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    test_loop(articles_original['validation'], nn_model, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "77b74a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nn_model(**{k:v for k,v in articles_original['train'].with_format('pt')[:50].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "3c8ff656",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(result.preds,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "dd17a0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "e9e94339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a93e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c914a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995c934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9fa8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e4567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "492fb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from Transformer_Models import ContextDistilBert, ContextDistilBertwithData\n",
    "import torch\n",
    "\n",
    "\n",
    "class ContextDistilBertwithDataforClassification(nn.Module):\n",
    "    def __init__(self, model_dir):\n",
    "        super().__init__()\n",
    "        self.body = ContextDistilBertwithData(model_dir)\n",
    "        self.pre_classifier = nn.Linear(1589, 1589)\n",
    "        self.classifier = nn.Linear(1589, 2)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        distilbert_input = {k:v for k,v in kwargs.items() if 'shares' not in k}\n",
    "        labels = kwargs['shares_class']\n",
    "        distilbert_output = self.body(**distilbert_input)\n",
    "        pooled_output = distilbert_output\n",
    "        pooled_output = self.pre_classifier(pooled_output)\n",
    "        pooled_output = nn.ReLU()(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        # calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
    "            \n",
    "        result = SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output\n",
    "        )\n",
    "        \n",
    "\n",
    "        # return model output object\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98530926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(lables, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall':recall, 'f1':f1, 'auc':auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12c3a8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0., grad_fn=<NllLossBackward0>), logits=tensor([[ 4735.8286, -1906.8658],\n",
       "        [ 4251.9004,  1840.8829]], grad_fn=<AddmmBackward0>), hidden_states=tensor([[-0.1956, -0.0263,  0.1797,  ...,  0.0000,  0.5000,  0.0000],\n",
       "        [-0.0854, -0.2529, -0.1250,  ...,  0.8000,  0.5000,  0.8000]],\n",
       "       grad_fn=<CatBackward0>), attentions=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**articles_encoded['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f12b109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ckpt = 'domain_adaptation_final_body'\n",
    "articles_encoded.set_format('torch')\n",
    "model = ContextDistilBertwithDataforClassification(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "batch_size = 12\n",
    "logging_steps = len(articles_encoded['train'])//batch_size\n",
    "model_name = \"finetuned-context_only\"\n",
    "training_args = TrainingArguments(output_dir = model_name,\n",
    "                                 num_train_epochs = 3,\n",
    "                                learning_rate = 2e-5,\n",
    "                                 per_device_train_batch_size = batch_size,\n",
    "                                 per_device_eval_batch_size = batch_size,\n",
    "                                 weight_decay = 0.01,\n",
    "                                 evaluation_strategy = 'epoch',\n",
    "                                 disable_tqdm = False,\n",
    "                                 logging_steps = logging_steps,\n",
    "                                 push_to_hub = False,\n",
    "                                 logging_dir = 'finetuned-context_only/finetuned_context_only_log'\n",
    "                                 fp16 = True,\n",
    "                                 resume_from_checkpoint = False,\n",
    "                                 load_best_model_at_end = True,\n",
    "                                 metric_for_best_model = 'auc')\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model = model, \n",
    "                 args = training_args,\n",
    "                 train_dataset = articles_encoded['train'].shuffle(),\n",
    "                 eval_dataset = articles_encoded['validation'].shuffle(),\n",
    "                  compute_metrics = compute_metrics,\n",
    "                  tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aad418d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"LDA_04, n_non_stop_unique_tokens, num_hrefs, kw_max_min, kw_avg_max, data_channel_is_entertainment, max_negative_polarity, kw_min_avg, kw_avg_min, kw_max_max, self_reference_avg_sharess, attention_mask_title, num_keywords, global_rate_positive_words, avg_negative_polarity, n_tokens_title, weekday_is_monday, num_self_hrefs, global_subjectivity, num_videos, rate_negative_words, min_positive_polarity, avg_positive_polarity, input_ids_title, kw_min_max, LDA_01, data_channel_is_tech, global_rate_negative_words, data_channel_is_bus, LDA_00, self_reference_min_shares, weekday_is_sunday, kw_min_min, kw_avg_avg, title_subjectivity, weekday_is_thursday, max_positive_polarity, LDA_03, weekday_is_friday, n_tokens_content, weekday_is_wednesday, min_negative_polarity, LDA_02, data_channel_is_lifestyle, abs_title_sentiment_polarity, rate_positive_words, average_token_length, title_sentiment_polarity, num_imgs, kw_max_avg, n_unique_tokens, shares_class, abs_title_subjectivity, data_channel_is_world, input_ids_content, attention_mask_content, data_channel_is_socmed, weekday_is_saturday, weekday_is_tuesday, global_sentiment_polarity, self_reference_max_shares\".split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed9fc2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LDA_04', 'n_non_stop_unique_tokens', 'num_hrefs', 'kw_max_min', 'kw_avg_max', 'data_channel_is_entertainment', 'max_negative_polarity', 'kw_min_avg', 'kw_avg_min', 'kw_max_max', 'self_reference_avg_sharess', 'attention_mask_title', 'num_keywords', 'global_rate_positive_words', 'avg_negative_polarity', 'n_tokens_title', 'weekday_is_monday', 'num_self_hrefs', 'global_subjectivity', 'num_videos', 'rate_negative_words', 'min_positive_polarity', 'avg_positive_polarity', 'input_ids_title', 'kw_min_max', 'LDA_01', 'data_channel_is_tech', 'global_rate_negative_words', 'data_channel_is_bus', 'LDA_00', 'self_reference_min_shares', 'weekday_is_sunday', 'kw_min_min', 'kw_avg_avg', 'title_subjectivity', 'weekday_is_thursday', 'max_positive_polarity', 'LDA_03', 'weekday_is_friday', 'n_tokens_content', 'weekday_is_wednesday', 'min_negative_polarity', 'LDA_02', 'data_channel_is_lifestyle', 'abs_title_sentiment_polarity', 'rate_positive_words', 'average_token_length', 'title_sentiment_polarity', 'num_imgs', 'kw_max_avg', 'n_unique_tokens', 'shares_class', 'abs_title_subjectivity', 'data_channel_is_world', 'input_ids_content', 'attention_mask_content', 'data_channel_is_socmed', 'weekday_is_saturday', 'weekday_is_tuesday', 'global_sentiment_polarity', 'self_reference_max_shares']\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8ed7b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['LDA_04', 'n_non_stop_unique_tokens', 'num_hrefs', 'kw_max_min', \n",
    "        'kw_avg_max', 'data_channel_is_entertainment', 'max_negative_polarity', 'kw_min_avg', \n",
    "        'kw_avg_min', 'kw_max_max', 'self_reference_avg_sharess',  \n",
    "        'num_keywords', 'global_rate_positive_words', 'avg_negative_polarity', 'n_tokens_title', \n",
    "        'weekday_is_monday', 'num_self_hrefs', 'global_subjectivity', 'num_videos', 'rate_negative_words', \n",
    "        'min_positive_polarity', 'avg_positive_polarity','kw_min_max', 'LDA_01', \n",
    "        'data_channel_is_tech', 'global_rate_negative_words', 'data_channel_is_bus', 'LDA_00', \n",
    "        'self_reference_min_shares', 'weekday_is_sunday', 'kw_min_min', 'kw_avg_avg', 'title_subjectivity', \n",
    "        'weekday_is_thursday', 'max_positive_polarity', 'LDA_03', 'weekday_is_friday', 'n_tokens_content', \n",
    "        'weekday_is_wednesday', 'min_negative_polarity', 'LDA_02', 'data_channel_is_lifestyle', \n",
    "        'abs_title_sentiment_polarity', 'rate_positive_words', 'average_token_length', \n",
    "        'title_sentiment_polarity', 'num_imgs', 'kw_max_avg', 'n_unique_tokens', 'shares_class', \n",
    "        'abs_title_subjectivity', 'data_channel_is_world', \n",
    "        'data_channel_is_socmed', 'weekday_is_saturday', 'weekday_is_tuesday', \n",
    "        'global_sentiment_polarity', 'self_reference_max_shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95230c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['n_tokens_title',\n",
    " 'n_tokens_content',\n",
    " 'n_unique_tokens',\n",
    " 'n_non_stop_unique_tokens',\n",
    " 'num_hrefs',\n",
    " 'num_self_hrefs',\n",
    " 'num_imgs',\n",
    " 'num_videos',\n",
    " 'average_token_length',\n",
    " 'num_keywords',\n",
    " 'data_channel_is_lifestyle',\n",
    " 'data_channel_is_entertainment',\n",
    " 'data_channel_is_bus',\n",
    " 'data_channel_is_socmed',\n",
    " 'data_channel_is_tech',\n",
    " 'data_channel_is_world',\n",
    " 'kw_min_min',\n",
    " 'kw_max_min',\n",
    " 'kw_avg_min',\n",
    " 'kw_min_max',\n",
    " 'kw_max_max',\n",
    " 'kw_avg_max',\n",
    " 'kw_min_avg',\n",
    " 'kw_max_avg',\n",
    " 'kw_avg_avg',\n",
    " 'self_reference_min_shares',\n",
    " 'self_reference_max_shares',\n",
    " 'self_reference_avg_sharess',\n",
    " 'weekday_is_monday',\n",
    " 'weekday_is_tuesday',\n",
    " 'weekday_is_wednesday',\n",
    " 'weekday_is_thursday',\n",
    " 'weekday_is_friday',\n",
    " 'weekday_is_saturday',\n",
    " 'weekday_is_sunday',\n",
    " 'LDA_00',\n",
    " 'LDA_01',\n",
    " 'LDA_02',\n",
    " 'LDA_03',\n",
    " 'LDA_04',\n",
    " 'global_subjectivity',\n",
    " 'global_sentiment_polarity',\n",
    " 'global_rate_positive_words',\n",
    " 'global_rate_negative_words',\n",
    " 'rate_positive_words',\n",
    " 'rate_negative_words',\n",
    " 'avg_positive_polarity',\n",
    " 'min_positive_polarity',\n",
    " 'max_positive_polarity',\n",
    " 'avg_negative_polarity',\n",
    " 'min_negative_polarity',\n",
    " 'max_negative_polarity',\n",
    " 'title_subjectivity',\n",
    " 'title_sentiment_polarity',\n",
    " 'abs_title_subjectivity',\n",
    " 'abs_title_sentiment_polarity',\n",
    " 'shares_class',\n",
    " 'input_ids_title',\n",
    " 'attention_mask_title',\n",
    " 'input_ids_content',\n",
    " 'attention_mask_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "181e42b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_title,\n",
      "n_tokens_content,\n",
      "n_unique_tokens,\n",
      "n_non_stop_unique_tokens,\n",
      "num_hrefs,\n",
      "num_self_hrefs,\n",
      "num_imgs,\n",
      "num_videos,\n",
      "average_token_length,\n",
      "num_keywords,\n",
      "data_channel_is_lifestyle,\n",
      "data_channel_is_entertainment,\n",
      "data_channel_is_bus,\n",
      "data_channel_is_socmed,\n",
      "data_channel_is_tech,\n",
      "data_channel_is_world,\n",
      "kw_min_min,\n",
      "kw_max_min,\n",
      "kw_avg_min,\n",
      "kw_min_max,\n",
      "kw_max_max,\n",
      "kw_avg_max,\n",
      "kw_min_avg,\n",
      "kw_max_avg,\n",
      "kw_avg_avg,\n",
      "self_reference_min_shares,\n",
      "self_reference_max_shares,\n",
      "self_reference_avg_sharess,\n",
      "weekday_is_monday,\n",
      "weekday_is_tuesday,\n",
      "weekday_is_wednesday,\n",
      "weekday_is_thursday,\n",
      "weekday_is_friday,\n",
      "weekday_is_saturday,\n",
      "weekday_is_sunday,\n",
      "LDA_00,\n",
      "LDA_01,\n",
      "LDA_02,\n",
      "LDA_03,\n",
      "LDA_04,\n",
      "global_subjectivity,\n",
      "global_sentiment_polarity,\n",
      "global_rate_positive_words,\n",
      "global_rate_negative_words,\n",
      "rate_positive_words,\n",
      "rate_negative_words,\n",
      "avg_positive_polarity,\n",
      "min_positive_polarity,\n",
      "max_positive_polarity,\n",
      "avg_negative_polarity,\n",
      "min_negative_polarity,\n",
      "max_negative_polarity,\n",
      "title_subjectivity,\n",
      "title_sentiment_polarity,\n",
      "abs_title_subjectivity,\n",
      "abs_title_sentiment_polarity,\n",
      "shares_class,\n",
      "input_ids_title,\n",
      "attention_mask_title,\n",
      "input_ids_content,\n",
      "attention_mask_content,\n"
     ]
    }
   ],
   "source": [
    "for x in columns:\n",
    "    print(f\"{x},\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c738a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0ff5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1cea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537099b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec67ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df2941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018f325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393addd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec44f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fdb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"attention_mask_title = None, input_ids_title = None, \n",
    "                    attention_mask_content = None, input_ids_content = None,\n",
    "                    LDA_04 = None,\n",
    "                    n_non_stop_unique_tokens = None,\n",
    "                    num_hrefs = None,\n",
    "                    kw_max_min = None,\n",
    "                    kw_avg_max = None,\n",
    "                    data_channel_is_entertainment = None,\n",
    "                    max_negative_polarity = None,\n",
    "                    kw_min_avg = None,\n",
    "                    kw_avg_min = None,\n",
    "                    kw_max_max = None,\n",
    "                    self_reference_avg_sharess = None,\n",
    "                    num_keywords = None,\n",
    "                    global_rate_positive_words = None,\n",
    "                    avg_negative_polarity = None,\n",
    "                    n_tokens_title = None,\n",
    "                    weekday_is_monday = None,\n",
    "                    num_self_hrefs = None,\n",
    "                    global_subjectivity = None,\n",
    "                    num_videos = None,\n",
    "                    rate_negative_words = None,\n",
    "                    min_positive_polarity = None,\n",
    "                    avg_positive_polarity = None,\n",
    "                    kw_min_max = None,\n",
    "                    LDA_01 = None,\n",
    "                    data_channel_is_tech = None,\n",
    "                    global_rate_negative_words = None,\n",
    "                    data_channel_is_bus = None,\n",
    "                    LDA_00 = None,\n",
    "                    self_reference_min_shares = None,\n",
    "                    weekday_is_sunday = None,\n",
    "                    kw_min_min = None,\n",
    "                    kw_avg_avg = None,\n",
    "                    title_subjectivity = None,\n",
    "                    weekday_is_thursday = None,\n",
    "                    max_positive_polarity = None,\n",
    "                    LDA_03 = None,\n",
    "                    weekday_is_friday = None,\n",
    "                    n_tokens_content = None,\n",
    "                    weekday_is_wednesday = None,\n",
    "                    min_negative_polarity = None,\n",
    "                    LDA_02 = None,\n",
    "                    data_channel_is_lifestyle = None,\n",
    "                    abs_title_sentiment_polarity = None,\n",
    "                    rate_positive_words = None,\n",
    "                    average_token_length = None,\n",
    "                    title_sentiment_polarity = None,\n",
    "                    num_imgs = None,\n",
    "                    kw_max_avg = None,\n",
    "                    n_unique_tokens = None,\n",
    "                    shares_class = None,\n",
    "                    abs_title_subjectivity = None,\n",
    "                    data_channel_is_world = None,\n",
    "                    data_channel_is_socmed = None,\n",
    "                    weekday_is_saturday = None,\n",
    "                    weekday_is_tuesday = None,\n",
    "                    global_sentiment_polarity = None,\n",
    "                    self_reference_max_shares = None\""
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
